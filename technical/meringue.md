
# 무중단 배포란?

배포 시의 애플리케이션의 다운타임을 극도록 낮추기 위한 기술입니다.

**빅뱅 배포** 방식의 문제점을 해결하기 위해 등장했습니다.

- **빅뱅 배포**: 배포 시에 기존 애플리케이션을 내리고, 새로운 애플리케이션을 올리는 방식입니다. 새로운 서버가 올라가는 동안 다운타임이 존재합니다.

# 무중단 배포를 해야 하는 이유

## **개발/협업 환경에서의 비효율성 해소**

현재 개발 및 테스트 환경에서, 세 번 중 한 번은 **“배포 중으로 인한 서버 다운”** 문제가 발생하고 있습니다.

곧바로 소통하기 힘든 상황에서 매번 디스코드를 통해 **“서버 문제 있나요?”** 를 묻고, 몇 분 뒤에 **"배포 중이라 서버가 내려갔습니다"** 와 같은 대답을 들어야 합니다.

이 방식은 협업 흐름을 끊고, 개발 시간 낭비를 유발하고 있습니다.

무중단 배포 도입을 통해 개발 과정에서의 비효율성을 제거하고, 프론트엔드/백엔드 간의 협업 신뢰도와 생산성을 극대화해야 합니다.

## **프로덕션 환경에서의 사용자 이탈 최소화**

현재 개발 환경에서 발생하는 다운타임 문제가 프로덕션 환경에서 동일하게 나타나고 있습니다.

다운타임 문제가 지속될 경우, 사용자에게 **"불안정한 서비스**"라는 인식을 심어주게 됩니다.

→ 사용자 이탈로 이어질 수 있습니다.

무중단 배포를 통해 다운타임을 극단적으로 줄여 고가용성을 확보하기 위해 노력해야 합니다.

고가용성은 사용자 경험 유지와 서비스 신뢰도로 직결되는 핵심 가치입니다.

# 무중단 배포 방식

## 스위치 배포

트래픽을 한 번에 전환(스위칭)하는 배포 방식입니다.

새로운 버전의 서버(들)를 실행시킨 후, 헬스체크를 통해 안정화 여부를 판단합니다.

안정적이라고 판단 시, 모든 트래픽을 한 번에 새로운 버전의 서버(들)로 스위칭합니다.

### 장점

1. **구현의 편의성**

   점진적 배포 방식에 비해 신경쓸 요소가 적어 구현이 간편합니다.


2. **빠른 롤백**

   문제 발생 시 즉시 모든 트래픽을 한 번에 이전 버전의 서버(들)로 스위칭할 수 있으므로, 롤백이 빠릅니다.


### 단점

1. **위험 부담**

   새로운 버전의 서버(들)에 런타임 문제 존재 시, 모든 사용자들이 영향받습니다.


2. **리소스 부담** 

   N대의 서버가 추가적으로 필요합니다.

   새로운 버전이 안정화되기까지 기존 버전의 서버들(N대)이 스탠바이 중 이어야 하기 때문입니다.


### 포트 스위칭 배포

하나의 서버에서 애플리케이션이 구동 중일 때 사용할 수 있는 방식입니다.

새로운 애플리케이션을 새로운 포트로 실행시킨 후, 리버스 프록시를 사용해 트래픽을 새로운 포트로 스위치하는 방식입니다.

<img src="image/port-switching-deployment.png" alt="port-switching-deployment.png" style="max-width: 80%; height: auto;">


블루/그린 배포라고 오해할 수 있지만, 근본적으로 다른 배포 방식입니다.

한 서버 인스턴스에서 두 버전의 애플리케이션이 동시에 구동되는 방식이기 때문입니다.

**시나리오 예시**

1. 443 → 8080으로 포트포워딩 중입니다.
2. 8081 포트로 새 애플리케이션을 실행한 후, 모든 테스트를 완료합니다.
3. 이 과정에서, 기존 애플리케이션은 정상적으로 실행되고 있습니다.
4. 새 애플리케이션이 안정적이라고 판단되면, 포트포워딩 설정을 변경합니다. (443 → 8081)
5. 기존 애플리케이션(8080)은 스탠바이 상태로 잠시 유지합니다.
6. 사용자들이 사용하며 별 문제가 없다고 판단 시 기존 애플리케이션(8080)을 제거합니다.

**장점**

1. **구현의 편의성**

   단순 포트 스위칭만으로 구현할 수 있어 구현이 매우 간단합니다.


**단점**

1. **명확한 한계점**

   단일 서버에서 두 개의 스프링 애플리케이션을 실행하는 시점이 존재합니다.

   현재, 하나의 스프링 애플리케이션은 약 20.4%의 메모리를 차지하고 있습니다.

    ```bash
    # 현재 총 메모리 사용량
    # 메모리: 1.8GiB 중 973MiB 사용 중 (약 52.79% 사용 중)
    # 스왑 메모리는 고려 X
                   total        used        free      shared  buff/cache   available
    Mem:           1.8Gi       973Mi        89Mi        20Mi       967Mi       869Mi
    Swap:          4.0Gi       643Mi       3.4Gi
    
    # 스프링 애플리케이션의 메모리 사용량 = 391160KiB = 381.99 MiB = 전체 메모리의 약 20.7%
    top -p {프로세스ID}
    
    PID    USER      PR   NI VIRT     RES     SHR    S   %CPU  %MEM     TIME+    COMMAND 
    804384 root      20   0  3203564  391160  23772  S   0.0   20.7     2:29.71  java
    ```

    스프링 애플리케이션을 하나 더 실행시키면, 메모리 사용률(%)이 약 52.79 → 약 73.51 이 됩니다. (약 20.7% 증가)

    73.51% 는 크게 문제될 사용률은 아니고 스왑 메모리도 존재하므로, 크게 문제되는 수치는 아닙니다.
    
    그러나, 서버를 점진적으로 교체해나가는 점진적 배포 방식을 사용하는 데에 제약이 생길 수 있습니다.
    
    점진적 배포 방식은 서버 클러스터링 환경을 기반으로 존재하는 방식입니다.
    
    점진적 배포 방식으로 무중단 배포를 구현하려면, 최소 4개의 스프링 애플리케이션이 필요합니다.
    
    4개의 스프링 애플리케이션을 실행하면, 메모리 사용률(%)이 약 52.79 → 약 114.89(100% 초과!) 가 됩니다. (약 62.1% 증가)
    
    **“I/O 작업으로 인해 느린”** 스왑 메모리가 필수로 필요하며, 애플리케이션이 활발해진다면 Heap 에 데이터가 쌓이게 되어 메모리 사용률이 더욱 증가할 것입니다.
    
    그러므로, 단일 서버 환경에서는 점진적 배포 방식은 현실적으로 사용하기 어렵습니다.


2. **기술 부채**

   서버 클러스터링 도입 시, 대부부을 다시 구현해야 합니다.

   Nginx 기반 포트 포워딩 방식 → 로드밸런서 기반 ip 분배 방식으로 변경되기 때문입니다.


3. **불완전한 격리**

   새로운 애플리케이션(8081)을 테스트하며 발생하는 문제가 전파되므로, 잠재적인 위험이 존재합니다.

   예) 새로운 애플리케이션(8081)에서 급작스럽게 메모리의 90%를 사용하게 되면, 서버 인스턴스 전체가 다운되어 기존 애플리케이션에도 영향을 미치게 됩니다.


### 블루/그린 배포

현재 운영 중인 환경과 동일한 새로운 환경을 구축하여 애플리케이션의 신규 버전을 배포하는 방식입니다.

기존에 실행되던 버전을 블루, 신규 버전을 그린이라 칭합니다.

<img src="image/blue-green-deployment.png" alt="blue-green-deployment.png" style="max-width: 80%; height: auto;">

**시나리오 예시**

1. 현재 트래픽을 처리하는 블루 환경과 동일한 인프라(EC2, 클러스터링 등)를 갖춘 그린 환경을 준비합니다.
2. 그린 환경에 새로운 버전을 배포하고, 모든 테스트를 완료합니다.
3. 이 과정에서, 블루 환경은 정상적으로 서비스되고 있습니다.
4. 새로운 버전이 안정적이라고 판단되면, 로드 밸런서의 설정을 변경하여 모든 사용자 트래픽을 블루 환경에서 그린 환경으로 전부(0%→100% 한 번에) 스위칭합니다.
5. 블루 환경은 스탠바이 상태로 잠시 유지합니다.
6. 사용자들이 사용하며 별 문제가 없다고 판단 시 블루 환경을 제거합니다.

**장점**

1. **완벽한 격리**

   그린 환경을 테스트하며 발생하는 문제가 블루 환경으로 전파되지 않기 때문에, 잠재적인 위험이 존재하지 않습니다.

   예) 그린 환경에서 급작스럽게 메모리의 90%를 사용하게 되어도, 블루 환경에는 영향을 미치지 않습니다.


**단점**

1. **비용**

   블루 환경을 준비하고, 실행하고, 스탠바이하는 동안 서버 인스턴스가 두 배로 필요하게 됩니다.


## 점진적 배포

점진적 배포는 새로운 버전을 전체 사용자에게 한 번에 노출하는 대신, 새로운 버전에게 전달되는 사용자 트래픽을 점진적으로 늘려가는 방식입니다.

### 장점

1. **위험 부담 최소화**

   트래픽을 한 번에 전환하지 않고 점진적으로 전환하므로, 문제가 발생해도 모든 사용자에게 영향을 미치지 않습니다.


2. **비용 부담 최소화**

   스위칭 배포와 달리 N대의 스탠바이 서버를 필요로 하지 않으므로, 비용 부담이 적습니다.

   (카나리 배포 사용 시 서버가 추가적으로 필요하지만, N보다 적은 수의 서버를 사용하고, 블루/그린 배포에 비해 업타임이 적습니다)


### 단점

1. **복잡한 구현**

   새 버전을 점진적으로 배포하고 모니터링하기 위한 복잡한 구현이 필요하다.


2. **하위 호환성 보장**

   같은 시점에 두 가지 버전의 애플리케이션이 실행되므로, API나 DB 스키마 변경사항이 하위 호환성을 보장하도록 설계해야 합니다.


### 카나리 배포

새로운 버전을 전체 사용자 중 아주 작은 비율(1%∼5% 등)에게만 먼저 노출하여, 문제가 없는지 실제 운영 환경에서 테스트하고 검증하는 방식입니다.

위험 부담을 극도로 최소화하고, 실제 사용자 데이터(에러율, 지연 시간)를 분석하여 배포를 계속해도 좋을지 의사결정을 내리는 방식입니다. (롤링 배포와의 가장 큰 차이점)

<img src="image/canary-deployment-1.png" alt="canary-deployment-1.png" style="max-width: 80%; height: auto;">
<img src="image/canary-deployment-2.png" alt="canary-deployment-2.png" style="max-width: 80%; height: auto;">

일반적으로 로드 밸런서를 통해 구현됩니다.

**시나리오 예시**

1. 적은 수의 노드를 가지는 새로운 클러스터를 만들어 새로운 버전을 배포하고, 카나리 그룹으로 지정합니다.
2. 로드 밸런서 설정을 변경하여 전체 트래픽의 일부만 카나리 그룹으로 보냅니다.
3. 카나리 그룹의 에러율, 응답 시간, CPU/Memory 사용률 등의 지표를 기존 그룹과 비교 분석합니다.
4. 안전하다고 판단되면 카나리 트래픽 비율을 점차 늘려 카나리 그룹의 노출 범위를 확대합니다.
5. 이 과정에서, 트래픽 비율과 더불어 카나리 그룹 내 노드 수도 증가시킵니다.
6. 100% 전환이 완료되면 기존 서버를 제거합니다.

**장점**

1. 가장 높은 안정성

   카나리 그룹을 지속적으로 모니터링하며 배포 진행/롤백 여부를 결정할 수 있습니다.

   새로운 버전에 문제가 생겨도 극히 일부의 사용자만이 영향받습니다.


2. A/B 테스트 가능

   A/B 테스트에 활용될 수 있습니다. 카나리 그룹을 모니터링하며 얻은 데이터를 통해 의사결정을 할 수 있습니다.


**단점**

1. 구현, 유지보수의 복잡성

   카나리 그룹을 모니터링하기 위한 아키텍처를 설계해야 합니다.

   다른 배포 방식에 비해 구현하기 매우 까다롭고, 유지보수가 복잡합니다.


2. 긴 배포 시간

   카나리 그룹을 모니터링하며 점진적으로 배포하기 때문에, 최종적으로 배포가 완료되는 시간이 가장 깁니다.

   안정성과 시간의 트레이드 오프입니다.


### 롤링 배포

롤링 배포는 클러스터 내에서 운영 중인 기존 노드들을 하나씩 또는 작은 그룹 단위로 새로운 버전의 노드로 순차적으로 교체해나가는 방식입니다. **이 과정에서 클러스터의 전체 규모는 유지됩니다.**

자원 효율성에 초점을 맞춘 배포 방식으로, 추가적인 서버 인스턴스를 필요로 하지 않습니다.

<img src="image/rolling-deployment.png" alt="rolling-deployment.png" style="max-width: 80%; height: auto;">

**시나리오 예시**

1. 새로운 노드를 생성한 후, 새로운 버전을 실행시킵니다.
2. 새로운 노드와 애플리케이션에 대한 헬스체크가 완료되면 새로운 노드를 로드밸런서에 등록합니다.
3. 기존 버전이 동작하고 있는 노드를 클러스터에서 제거, 종료합니다.
4. 1~3을 반복합니다.
5. 최종적으로, 모든 노드가 새로운 버전의 노드로 교체됩니다.

**장점**

1. 자원 효율성

   다른 배포 방식과 달리, 추가적인 서버 인스턴스를 필요로 하지 않습니다.

   서버 인스턴스는 잠시동안 한 대만 필요합니다.


**단점**

1. 불완전하고 느린 롤백

   새로운 버전에서 문제 발생 시, 이전 버전과 똑같은 인프라로 롤백할 수 없습니다.

   총 노드 수: N

   기존 버전 노드 수: K

   새로운 버전 노드 수: M

   새로운 버전의 노드에서 문제 발생 시, 새로운 버전의 노드를 전부 제거 후 M개의 기존 버전 노드를 다시 생성해야 합니다.

   긴 시간이 소요되며, 남은 K개의 노드가 오랜 시간 동안 모든 요청을 처리해야 합니다.


# 어떤 배포 방식을 사용할 것인가?

현재, 루티는 단일 EC2에서 Nginx와 Spring 애플리케이션이 실행 중입니다.

<img src="image/routie-infra.png" alt="routie-infra.png" style="max-width: 60%; height: auto;">

인프라 구조 변경 시 리소스가 많이 소모된다는 것을 인지해야 합니다.

## 고려해야 할 사항

### 1. 비용(2025.09 기준)

애플리케이션용 인스턴스는 약 15$의 비용을 요구합니다.

9월 월 평균 비용: 38.52$

최대 사용 가능 비용: 70$

→ 추가 사용 가능 비용: **약 30$**

30$ 내에서 무중단 배포를 구현해야 합니다.

### 2. 구현 리소스

한정된 시간(7일) 내에 무중단 배포를 구현해야 합니다.

### 3. 미래 인프라 대규모 이동

약 3주 안에 우아한테크코스의 aws에서 우리 프로젝트의 모든 리소스를 제거해야 합니다.

현재 모든 것을 완벽히 구축하기보다는 무중단 배포 개념을 충족시킬 수 있는 수준으로만 구현하는 것이 적절합니다.

## 배포 방식 결정 - 포트 스위칭 배포

고려 사항을 통해 결정한 최종 배포 방식은 포트 스위칭 배포입니다.

각 고려 사항 별 어떤 점들을 고려했는지 서술하겠습니다.

## 이유

### 1. 비용 측면(2025.09 기준)

추가적인 비용이 거의 들지 않습니다. EC2 메모리 사용량이 잠시 증가할 뿐입니다.

만약 다른 방식의 배포를 사용하기 위해서는 서버를 격리해야 하고, 로드밸런서가 필수적으로 필요하게 됩니다.

애플리케이션용 인스턴스는 약 15$의 비용을 요구합니다.

로드밸런서의 비용을 10$라고 산정하겠습니다.

최소 인스턴스 수(기존 서버 한 대, 새 버전 서버 한 대, 로드밸런서 한 대)를 기준으로 총 25$의 추가 비용이 요구됩니다.

**한정된 비용(30$) 내에서 구현하기 위해서는 포트 스위칭 배포 방식이 최선입니다.**

### 2. 구현 리소스 측면

배포 방식을 조사하며 구현에 필요한 리소스를 산정해 보았습니다.

1. **포트 스위칭 배포**

   **2~3일 내**에 구현할 수 있습니다. 현재 Nginx 설정이 완료되어 있는 상황이고, 차니가 구현해 놓은 애플리케이션 실행, 복구 스크립트를 재활용할 수 있기 때문입니다.


2. **블루/그린 배포**

   클러스터를 구축해야 하며, 노드 자동 생성, 설정 초기화 아키텍처를 구축해야 합니다. (AWS AMI, ASG, EC2 User Data 등)

   경험이 없는 상태에서 레퍼런스를 참고해 구현하기 위해서는 **7일이 소요**될 것으로 예상됩니다.


3. **카나리 배포**

   클러스터를 구축해야 하며, 노드 자동 생성, 설정 초기화 아키텍처를 구축해야 합니다. (AWS AMI, ASG, EC2 User Data 등)

   이와 더불어, 모니터링 시스템과 점진적 배포 시스템을 설계하고 구축해야 합니다.

   학습해야 할 것이 너무 많기 때문에, **30일 이상 소요**될 것으로 예상됩니다.


4. **롤링 배포**

   클러스터를 구축해야 하며, 노드 자동 생성, 설정 초기화 아키텍처를 구축해야 합니다. (AWS AMI, ASG, EC2 User Data 등)

   사용자 활동 모니터링이나 정교한 점진적 배포 시스템은 구현하지 않아도 됩니다.

   경험이 없는 상태에서 레퍼런스를 참고해 구현하기 위해서는 **9일이 소요**될 것으로 예상됩니다.


**정리**

|   배포 전략    | 클러스터 구축 여부 | 구현 복잡성 | 예상 소요일 |
|:----------:|:----------:|:------:|:------:|
| 포트 스위칭 배포  |     X      |   하    |  2~3일  |
|  블루/그린 배포  |     O      |   중    |   7일   |
|   카나리 배포   |     O      |   상    | 30일 이상 |
|   롤링 배포    |     O      |   중    |   9일   |


일정을 여유롭게 잡아야 하기 때문에, **제한된 시간(7일) 내에 구현하기 가장 적절한 방식은 포트 스위칭 배포** 방식이라 판단했습니다.

### 3. 미래 인프라 대규모 이동 측면

약 3주 안에 우아한테크코스의 aws에서 우리 프로젝트의 모든 리소스를 제거해야 합니다.

3주 동안 복잡한 인프라를 구축하는 데에 시간을 쏟아도, 결국 전부 제거하고 다시 구성해야 합니다.

**“구축해 본 경험”** 측면에서 나쁜 선택은 아닙니다.

- 추후 인프라 재건 시 도움이 되는 경험입니다.

그러나, **“구축해 본 경험”** 을 얻기 위해 사용되는 기회비용이 큽니다.

비즈니스 로직 성능 개선과 기존 로직 고도화 등, 우리 팀은 해결해야 하는 문제가 많습니다.

우리 팀이 당장 해결해야 하는 문제와 “구축해 본 경험”을 비교해봤을 때, **우리 팀이 당장 해결해야 하는 문제를 먼저 해결하는 편이 더 적절** 하다고 생각합니다.
